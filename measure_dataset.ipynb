{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(\"./enigma-transformed/src\")\n",
    "sys.path.append(\"./src\")\n",
    "sys.path.append(\"../src\")\n",
    "sys.path.append(\"../../src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import jensenshannon\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "def js_divergence(dict1, dict2):\n",
    "    \"\"\"Computes JS divergence of two dictionaries of counts converted to probability distributions  \n",
    "    Input:\n",
    "        dict1: dictionary of counts\n",
    "        dict2: dictionary of counts\n",
    "    \"\"\"\n",
    "    # Get the union of keys from both dictionaries\n",
    "    keys = set(dict1.keys()).union(set(dict2.keys()))\n",
    "\n",
    "    # Convert frequency dictionaries to probability distributions\n",
    "    total1 = sum(dict1.values())\n",
    "    total2 = sum(dict2.values())\n",
    "    prob_dist1 = [dict1.get(key, 0) / total1 for key in keys]\n",
    "    prob_dist2 = [dict2.get(key, 0) / total2 for key in keys]\n",
    "\n",
    "    # Calculate Jensen-Shannon divergence\n",
    "    divergence = jensenshannon(prob_dist1, prob_dist2)\n",
    "\n",
    "    return divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "import spacy\n",
    "import pandas as pd\n",
    "# what functions are avaliable to measure?\n",
    "\n",
    "def compute_typical_distribution(dataframe, column, aggregation_fn):\n",
    "    pass\n",
    "\n",
    "# 1. unigram js_divergence\n",
    "# 2. bpe\n",
    "# 3. bigram js_divergence\n",
    "# 4. gpt2 perplexity\n",
    "# 5. depth of parse tree\n",
    "# 6. js_divergence of POS tags\n",
    "# 7. js_divergence of POS bigrams\n",
    "# 8. sum inverse frequencies of words in the language\n",
    "# 9. number of named entities / presence of them\n",
    "# ?trigram js_divergence does this make sense for 200tok\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_gpt2_perplexity():\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "    device = (\n",
    "        torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    )\n",
    "    gpt2 = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n",
    "\n",
    "    def gpt2_perplexity(text):\n",
    "        # Encode and prepare inputs\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        # Calculate log likelihood\n",
    "        with torch.no_grad():\n",
    "            outputs = gpt2(**inputs, labels=inputs[\"input_ids\"])\n",
    "        log_likelihood = outputs.loss.item()\n",
    "\n",
    "        # Calculate perplexity\n",
    "        perplexity = torch.exp(torch.tensor(log_likelihood)).item()\n",
    "\n",
    "        return perplexity\n",
    "\n",
    "\n",
    "def create_bpe_tokens_per_char():\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "    def bpe_tokens_per_char(text):\n",
    "        chars = len(text)\n",
    "        tokens = len(tokenizer.encode(text))\n",
    "        return tokens / chars\n",
    "    return bpe_tokens_per_char\n",
    "\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "def create_unigram_js_divergence(data):\n",
    "    # compute the typical distribution\n",
    "    counts = Counter()\n",
    "    for text in data.text:\n",
    "        counts.update(text)   \n",
    "\n",
    "\n",
    "    def unigram_js_divergence(text):\n",
    "        return js_divergence(counts, Counter(text))\n",
    "\n",
    "    return unigram_js_divergence\n",
    "\n",
    "\n",
    "def create_bigram_js_divergence(data):\n",
    "    counts = Counter()\n",
    "    for text in data.text:\n",
    "        counts.update(zip(text, text[1:]))\n",
    "    def bigram_js_divergence(text):\n",
    "        return js_divergence(counts, Counter(zip(text, text[1:])))\n",
    "    return bigram_js_divergence\n",
    "\n",
    "\n",
    "def create_pos_js_divergence(data):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    counts = Counter()\n",
    "    for text in data.original_text:\n",
    "        doc = nlp(text)\n",
    "        pos = [token.pos_ for token in doc]\n",
    "        counts.update(pos)\n",
    "\n",
    "    def pos_js_divergence(text):\n",
    "        doc = nlp(text)\n",
    "        pos = [token.pos_ for token in doc]\n",
    "        return js_divergence(counts, Counter(pos))\n",
    "    return pos_js_divergence\n",
    "\n",
    "\n",
    "def create_pos_bigram_js_divergence(data):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    counts = Counter()\n",
    "    for text in data.original_text:\n",
    "        doc = nlp(text)\n",
    "        pos = [token.pos_ for token in doc]\n",
    "        counts.update(zip(pos, pos[1:]))\n",
    "\n",
    "    def pos_bigram_js_divergence(text):\n",
    "        doc = nlp(text)\n",
    "        pos = [token.pos_ for token in doc]\n",
    "        return js_divergence(counts, Counter(zip(pos, pos[1:])))\n",
    "    \n",
    "    return pos_bigram_js_divergence\n",
    "\n",
    "def create_depth_of_parse_tree():\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    def depth_of_parse_tree(text):\n",
    "        doc = nlp(text)\n",
    "        def find_depth(node):\n",
    "            if not list(node.children):\n",
    "                return 1\n",
    "            else:\n",
    "                return 1 + max(find_depth(child) for child in node.children)\n",
    "\n",
    "        # Finding the root of the parse tree\n",
    "        root = [token for token in doc if token.head == token][0]\n",
    "        return find_depth(root)\n",
    "\n",
    "    return depth_of_parse_tree\n",
    "\n",
    "\n",
    "def create_named_entities():\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    def named_entities(text):\n",
    "        doc = nlp(text)\n",
    "        return len(doc.ents)\n",
    "    return named_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# ne = create_bigram_js_divergence([\"The quick brown fox jumped over the lazy dog, and then the dog ate the fox, and then aaaa, Paris, Knowllledge, kedrchleba, aadasdfadf, afas f ff  eee ewerw w errrr re e eeeeee e err r  rr r rrrrrrre e  \"])\n",
    "# start = time.time()\n",
    "# ne(\"The quick brown fox jumped over the lazy dog, and then the dog ate the fox, and then aaaa, Paris, Knowllledge, kedrchleba, aadasdfadf, afas f ff  eee ewerw w errrr re e eeeeee e err r  rr r rrrrrrre e  \")\n",
    "# ne(\"aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\")\n",
    "# ne(\"aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\")\n",
    "# ne(\"aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\")\n",
    "# ne(\"aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\")\n",
    "# ne(\"aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\")\n",
    "# ne(\"aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\")\n",
    "# ne(\"aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\")\n",
    "# ne(\"aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\")\n",
    "# end = time.time()\n",
    "# t = end - start\n",
    "# print(t)\n",
    "# print(t*5000000/60/60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"news.2013.en.trainlen.200\")\n",
    "# data = pd.read_csv(\"news.test.trainlen200\")\n",
    "available_functions = [\n",
    "    # (\"gpt2_perplexity\", create_gpt2_perplexity(), 'text'), # done\n",
    "    # (\"bpe_tokens_per_char\", create_bpe_tokens_per_char(), 'text'), # done\n",
    "    # (\"unigram_js_divergence\", create_unigram_js_divergence(data), 'text'), #done\n",
    "    # (\"bigram_js_divergence\", create_bigram_js_divergence(data), 'text'),#17648\n",
    "    # (\"pos_js_divergence\", create_pos_js_divergence(data),'original_text'), #17650\n",
    "    (\"pos_bigram_js_divergence\", create_pos_bigram_js_divergence(data), 'original_text'), #17655\n",
    "    # (\"depth_of_parse_tree\", create_depth_of_parse_tree(), 'original_text'),#17652\n",
    "    # (\"named_entities\", create_named_entities(),'original_text')#17649\n",
    "]\n",
    "# TODO: fix this so it does not compute all closures every time\n",
    "\n",
    "processing_now=0\n",
    "fn_name, function, src_col = available_functions[processing_now]\n",
    "print(f\"Processing {fn_name}\")\n",
    "\n",
    "# function = bpe_tokens_per_char\n",
    "data[fn_name] = data[src_col].apply(lambda text: function(text))\n",
    "# data[\"gpt2_tokens_per_char\"] = data[\"text\"].apply(lambda text: function(text))\n",
    "\n",
    "data.to_csv(f\"news.2013.en.trainlen.200.{fn_name}\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "enigmavenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
