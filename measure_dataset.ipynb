{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(\"./enigma-transformed/src\")\n",
    "sys.path.append(\"./src\")\n",
    "sys.path.append(\"../src\")\n",
    "sys.path.append(\"../../src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import spacy_udpipe\n",
    "# from spacy_udpipe import  \n",
    "def english_process():\n",
    "    pass\n",
    "\n",
    "def german_process():\n",
    "    # load spacy medium model\n",
    "    nlp = spacy.load(\"de_core_news_md\")\n",
    "    def nlpize(text):\n",
    "        return nlp(text)\n",
    "\n",
    "    return nlpize\n",
    "\n",
    "\n",
    "def czech_process():\n",
    "    # NER has to be done externally \n",
    "    pass\n",
    "# g = german_process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded pre-trained UDPipe model for 'cs' language\n"
     ]
    }
   ],
   "source": [
    "# spacy_udpipe.download(\"cs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# nlp = spacy_udpipe.load(\"cs\")\n",
    "# nlp.analyze_pipes()\n",
    "\n",
    "# nlp._components\n",
    "\n",
    "# o = nlp(\"Ahoj světe, Karlův most, Brno, Paříž!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ahoj ahoj PART nmod světe\n",
      "světe svět NOUN ROOT světe\n",
      ", , PUNCT punct most\n",
      "Karlův Karlův ADJ amod most\n",
      "most most NOUN appos světe\n",
      ", , PUNCT punct Brno\n",
      "Brno Brno PROPN conj most\n",
      ", , PUNCT punct Paříž\n",
      "Paříž Paříž PROPN conj most\n",
      "! ! PUNCT punct světe\n"
     ]
    }
   ],
   "source": [
    "# for token in o:\n",
    "#     print(token.text, token.lemma_, token.pos_, token.dep_, token.head.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import jensenshannon\n",
    "from src.evaluation import js_divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "import spacy\n",
    "import pandas as pd\n",
    "# what functions are avaliable to measure?\n",
    "\n",
    "# 1. unigram js_divergence\n",
    "# 2. bpe\n",
    "# 3. bigram js_divergence\n",
    "# 4. gpt2 perplexity\n",
    "# 5. depth of parse tree\n",
    "# 6. js_divergence of POS tags\n",
    "# 7. js_divergence of POS bigrams\n",
    "# 8. number of named entities \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_gpt2_perplexity():\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "    device = (\n",
    "        torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    )\n",
    "    gpt2 = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n",
    "\n",
    "    def gpt2_perplexity(text):\n",
    "        # Encode and prepare inputs\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        # Calculate log likelihood\n",
    "        with torch.no_grad():\n",
    "            outputs = gpt2(**inputs, labels=inputs[\"input_ids\"])\n",
    "        log_likelihood = outputs.loss.item()\n",
    "\n",
    "        # Calculate perplexity\n",
    "        perplexity = torch.exp(torch.tensor(log_likelihood)).item()\n",
    "\n",
    "        return perplexity\n",
    "    return gpt2_perplexity\n",
    "\n",
    "\n",
    "def create_bpe_tokens_per_char():\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "    def bpe_tokens_per_char(text):\n",
    "        chars = len(text)\n",
    "        tokens = len(tokenizer.encode(text))\n",
    "        return tokens / chars\n",
    "    return bpe_tokens_per_char\n",
    "\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "def create_unigram_js_divergence(data):\n",
    "    # compute the typical distribution\n",
    "    counts = Counter()\n",
    "    for text in data.text:\n",
    "        counts.update(text)   \n",
    "\n",
    "\n",
    "    def unigram_js_divergence(text):\n",
    "        return js_divergence(counts, Counter(text))\n",
    "\n",
    "    return unigram_js_divergence\n",
    "\n",
    "\n",
    "def create_bigram_js_divergence(data):\n",
    "    counts = Counter()\n",
    "    for text in data.text:\n",
    "        counts.update(zip(text, text[1:]))\n",
    "    def bigram_js_divergence(text):\n",
    "        return js_divergence(counts, Counter(zip(text, text[1:])))\n",
    "    return bigram_js_divergence\n",
    "\n",
    "\n",
    "def create_pos_js_divergence(data):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    counts = Counter()\n",
    "    for text in data.original_text:\n",
    "        doc = nlp(text)\n",
    "        pos = [token.pos_ for token in doc]\n",
    "        counts.update(pos)\n",
    "\n",
    "    def pos_js_divergence(text):\n",
    "        doc = nlp(text)\n",
    "        pos = [token.pos_ for token in doc]\n",
    "        return js_divergence(counts, Counter(pos))\n",
    "    return pos_js_divergence\n",
    "\n",
    "\n",
    "def create_pos_bigram_js_divergence(data):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    counts = Counter()\n",
    "    for text in data.original_text:\n",
    "        doc = nlp(text)\n",
    "        pos = [token.pos_ for token in doc]\n",
    "        counts.update(zip(pos, pos[1:]))\n",
    "\n",
    "    def pos_bigram_js_divergence(text):\n",
    "        doc = nlp(text)\n",
    "        pos = [token.pos_ for token in doc]\n",
    "        return js_divergence(counts, Counter(zip(pos, pos[1:])))\n",
    "    \n",
    "    return pos_bigram_js_divergence\n",
    "\n",
    "def create_depth_of_parse_tree(): #??? how to validate that this computes valid things, subset and compute if results differ with different spacy models\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    def depth_of_parse_tree(text):\n",
    "        doc = nlp(text)\n",
    "        def find_depth(node):\n",
    "            if not list(node.children):\n",
    "                return 1\n",
    "            else:\n",
    "                return 1 + max(find_depth(child) for child in node.children)\n",
    "\n",
    "        # Finding the root of the parse tree\n",
    "        root = [token for token in doc if token.head == token][0]\n",
    "        return find_depth(root)\n",
    "\n",
    "    return depth_of_parse_tree\n",
    "\n",
    "\n",
    "def create_named_entities():\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    def named_entities(text):\n",
    "        doc = nlp(text)\n",
    "        return len(doc.ents)\n",
    "    return named_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"news.2013.en.trainlen.200\")\n",
    "# data = pd.read_csv(\"news.test.trainlen200\")\n",
    "available_functions = [\n",
    "    # (\"gpt2_perplexity\", create_gpt2_perplexity(), 'text'), # done\n",
    "    # (\"gpt2_tokens_per_char\", create_bpe_tokens_per_char(), 'text'), # done\n",
    "    # (\"unigram_js_divergence\", create_unigram_js_divergence(data), 'text'), #done\n",
    "    # (\"bigram_js_divergence\", create_bigram_js_divergence(data), 'text'),#17648\n",
    "    # (\"pos_js_divergence\", create_pos_js_divergence(data),'original_text'), #17650\n",
    "    (\"pos_bigram_js_divergence\", create_pos_bigram_js_divergence(data), 'original_text'), #17655\n",
    "    # (\"depth_of_parse_tree\", create_depth_of_parse_tree(), 'original_text'),#17652\n",
    "    # (\"named_entities\", create_named_entities(),'original_text')#17649\n",
    "]\n",
    "\n",
    "processing_now=0\n",
    "fn_name, function, src_col = available_functions[processing_now]\n",
    "print(f\"Processing {fn_name}\")\n",
    "\n",
    "# function = bpe_tokens_per_char\n",
    "data[fn_name] = data[src_col].apply(lambda text: function(text))\n",
    "# data[\"gpt2_tokens_per_char\"] = data[\"text\"].apply(lambda text: function(text))\n",
    "\n",
    "data.to_csv(f\"news.2013.en.trainlen.200.{fn_name}\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "enigmavenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
