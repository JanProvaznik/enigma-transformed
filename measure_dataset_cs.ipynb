{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import requests\n",
    "import json\n",
    "from collections import Counter\n",
    "import torch\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from src.evaluation import js_divergence\n",
    "# what functions are avaliable to measure?\n",
    "\n",
    "# 1. unigram js_divergence\n",
    "# 2. bpe\n",
    "# 3. bigram js_divergence\n",
    "# 4. gpt2 perplexity\n",
    "# 5. depth of parse tree\n",
    "# 6. js_divergence of POS tags\n",
    "# 7. js_divergence of POS bigrams\n",
    "# 8. number of named entities \n",
    "\n",
    "def create_gpt2_perplexity():\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "    device = (\n",
    "        torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    )\n",
    "    gpt2 = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n",
    "\n",
    "    def gpt2_perplexity(text):\n",
    "        # Encode and prepare inputs\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        # Calculate log likelihood\n",
    "        with torch.no_grad():\n",
    "            outputs = gpt2(**inputs, labels=inputs[\"input_ids\"])\n",
    "        log_likelihood = outputs.loss.item()\n",
    "\n",
    "        # Calculate perplexity\n",
    "        perplexity = torch.exp(torch.tensor(log_likelihood)).item()\n",
    "\n",
    "        return perplexity\n",
    "    return gpt2_perplexity\n",
    "\n",
    "\n",
    "def create_bpe_tokens_per_char():\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "    def bpe_tokens_per_char(text):\n",
    "        chars = len(text)\n",
    "        tokens = len(tokenizer.encode(text))\n",
    "        return tokens / chars\n",
    "    return bpe_tokens_per_char\n",
    "\n",
    "\n",
    "def find_depth(node):\n",
    "    if not list(node.children):\n",
    "        return 1\n",
    "    else:\n",
    "        return 1 + max(find_depth(child) for child in node.children)\n",
    "\n",
    "\n",
    "def create_all_nlp_functions_de(data):\n",
    "    nlp = spacy.load(\"de_core_news_md\")\n",
    "    counts_bigram_pos = Counter()\n",
    "    counts_pos = Counter()\n",
    "    for text in data.original_text:\n",
    "        doc = nlp(text)\n",
    "        pos = [token.pos_ for token in doc]\n",
    "        counts_pos.update(pos)\n",
    "        counts_bigram_pos.update(zip(pos, pos[1:]))\n",
    "\n",
    "    def inner(text):\n",
    "        doc = nlp(text)\n",
    "        pos = [token.pos_ for token in doc]\n",
    "        pos_js_divergence = js_divergence(counts_pos, Counter(pos))\n",
    "        pos_bigram_js_divergence = js_divergence(counts_bigram_pos, Counter(zip(pos, pos[1:])))\n",
    "        root = [token for token in doc if token.head == token][0]\n",
    "        depth = find_depth(root)\n",
    "        named_entities = len(doc.ents)\n",
    "\n",
    "        \n",
    "        return pos_js_divergence, pos_bigram_js_divergence, depth, named_entities\n",
    "    return inner\n",
    "\n",
    "import spacy_udpipe\n",
    "def create_all_nlp_functions_cs(data):\n",
    "    nlp = spacy_udpipe.load(\"cs\")\n",
    "    counts_bigram_pos = Counter()\n",
    "    counts_pos = Counter()\n",
    "    for text in data.original_text:\n",
    "        doc = nlp(text)\n",
    "        pos = [token.pos_ for token in doc]\n",
    "        counts_pos.update(pos)\n",
    "        counts_bigram_pos.update(zip(pos, pos[1:]))\n",
    "    def inner(text):\n",
    "        doc = nlp(text)\n",
    "        pos = [token.pos_ for token in doc]\n",
    "        pos_js_divergence = js_divergence(counts_pos, Counter(pos))\n",
    "        pos_bigram_js_divergence = js_divergence(counts_bigram_pos, Counter(zip(pos, pos[1:])))\n",
    "        root = [token for token in doc if token.head == token][0]\n",
    "        depth = find_depth(root)\n",
    "        # named_entities = len(doc.ents)\n",
    "        return pos_js_divergence, pos_bigram_js_divergence, depth\n",
    "    return inner\n",
    "def cs_named_entities(text):\n",
    "    # use REST server api\n",
    "    # if I were using curl it would go like this:\n",
    "    # curl -F 'output=vertical' --data-urlencode 'data={text}' http://lindat.mff.cuni.cz/services/nametag/api/recognize\n",
    "    url = \"http://lindat.mff.cuni.cz/services/nametag/api/recognize\"\n",
    "    data = {\"output\": \"vertical\", \"data\": text}\n",
    "    response = requests.post(url, data=data)\n",
    "    print(response.text)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        res = json.loads(response.text)['result']\n",
    "        print(res)\n",
    "        return len(res.split(\"\\n\")) - 1\n",
    "        # return len(response.text.split(\"\\n\")) - 1\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "def create_char_bigram_divergences(data):\n",
    "    unigram_counts = Counter()\n",
    "    bigram_counts = Counter()\n",
    "    for text in data.text:\n",
    "        unigram_counts.update(text)\n",
    "        bigram_counts.update(zip(text, text[1:]))\n",
    "    def inner(text):\n",
    "        unigram_divergence = js_divergence(unigram_counts, Counter(text))\n",
    "        bigram_divergence = js_divergence(bigram_counts, Counter(zip(text, text[1:])))\n",
    "        return unigram_divergence, bigram_divergence\n",
    "    return inner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fn created\n",
      "logo httpwwwprnasiacomsajpg logo httpwwwprnasiacomsajpg photo httpwwwprnasiacomsahtml\n",
      "(0.6876786758803325, 0.7994994041680255, 5, 0)\n",
      "photo httpphotosprnewswirecomprnhcga photo httpphotosprnewswirecomprnhcgb photo httpphotosprnewswirecomprnhcgc\n",
      "(0.6910429983207507, 0.8018178266332621, 5, 0)\n",
      "photo httpphotosprnewswirecomprnhnya photo httpphotosprnewswirecomprnhnyb photo httpphotosprnewswirecomprnhnyc\n",
      "(0.6931917126518249, 0.8024073260475586, 5, 0)\n",
      "photo httpphotosprnewswirecomprnhfla photo httpphotosprnewswirecomprnhflb photo httpphotosprnewswirecomprnhflc\n",
      "(0.6876786758803325, 0.7970752184481225, 5, 0)\n",
      "photo httpphotosprnewswirecomprnhnya logo httpphotosprnewswirecomprnhnylogob logo httpphotosprnewswirecomprnhnylogob\n",
      "(0.6931917126518249, 0.8024073260475586, 4, 0)\n",
      "photo httpphotosprnewswirecomprnha photo httpphotosprnewswirecomprnhb photo httpphotosprnewswirecomprnhc\n",
      "(0.6931917126518249, 0.8024073260475586, 5, 0)\n",
      "photo httpphotosprnewswirecomprnhnya photo httpphotosprnewswirecomprnhnyb logo httpphotosprnewswirecomprnhnylogo\n",
      "(0.6931917126518249, 0.8024073260475586, 3, 0)\n",
      "photo httpphotosprnewswirecomprnhnya photo httpphotosprnewswirecomprnhnyb logo httpphotosprnewswirecomprnhny\n",
      "(0.6931917126518249, 0.8024073260475586, 4, 0)\n",
      "photo httpphotosprnewswirecomprnhnya photo httpphotosprnewswirecomprnhnyb logo httpphotosprnewswirecomprnhnylogob\n",
      "(0.6931917126518249, 0.8024073260475586, 4, 0)\n",
      "wednesday february saturday february sunday february saturday february sunday february\n",
      "(0.7222709930564637, 0.8191811247180818, 2, 0)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('minidataset.csv').head(100)\n",
    "f = create_all_nlp_functions_cs(data)\n",
    "print('fn created')\n",
    "i = 0\n",
    "for text in data.text:\n",
    "    print(text)\n",
    "    print(f(text))\n",
    "    i+=1\n",
    "    if i == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"news.2013.cs.trainlen.200.csv\")\n",
    "# data = pd.read_csv(\"news.test.de.csv\")\n",
    "available_functions = [\n",
    "    # (\"gpt2_perplexity\", create_gpt2_perplexity(), 'text'), # done\n",
    "    # (\"bpe_tokens_per_char\", create_bpe_tokens_per_char(), 'text'), # done\n",
    "    # (\"unigram_js_divergence\", create_unigram_js_divergence(data), 'text'), #done\n",
    "    # (\"bigram_js_divergence\", create_bigram_js_divergence(data), 'text'),#17648\n",
    "    # (\"pos_js_divergence\", create_pos_js_divergence(data),'original_text'), #17650\n",
    "    # (\"pos_bigram_js_divergence\", create_pos_bigram_js_divergence(data), 'original_text'), #17655\n",
    "    # (\"depth_of_parse_tree\", create_depth_of_parse_tree(), 'original_text'),#17652\n",
    "    # (\"named_entities\", create_named_entities(),'original_text')#17649\n",
    "]\n",
    "\n",
    "# nlp_fn = create_all_nlp_functions_cs(data)\n",
    "# print(\"nlp fn created\")\n",
    "# data[\"pos_js_divergence\"], data[\"pos_bigram_js_divergence\"], data[\"depth_of_parse_tree\"] = zip(*data.original_text.apply(lambda text: nlp_fn(text)))\n",
    "\n",
    "# compute char divergences\n",
    "# char_divergences = create_char_bigram_divergences(data)\n",
    "# print(\"fn created\")\n",
    "# data[\"unigram_js_divergence\"], data[\"bigram_js_divergence\"] = zip(*data.text.apply(lambda text: char_divergences(text)))\n",
    "\n",
    "# compute ner\n",
    "# ner = cs_named_entities\n",
    "# print('computing ner')\n",
    "# data[\"named_entities\"] = data.original_text.apply(lambda text: ner(text))\n",
    "\n",
    "# compute gpt2 perplexity\n",
    "gpt2_perplexity = create_gpt2_perplexity()\n",
    "print(\"fn created\")\n",
    "data[\"gpt2_perplexity\"] = data.text.apply(lambda text: gpt2_perplexity(text))\n",
    "\n",
    "# compute bpe tokens per char\n",
    "# bpe_tokens_per_char = create_bpe_tokens_per_char()\n",
    "# print(\"fn created\")\n",
    "# data[\"bpe_tokens_per_char\"] = data.text.apply(lambda text: bpe_tokens_per_char(text))\n",
    "\n",
    "\n",
    "# print data\n",
    "# processing_now=0\n",
    "# fn_name, function, src_col = available_functions[processing_now]\n",
    "# print(f\"Processing {fn_name}\")\n",
    "\n",
    "# function = bpe_tokens_per_char\n",
    "# data[fn_name] = data[src_col].apply(lambda text: function(text))\n",
    "# data[\"gpt2_tokens_per_char\"] = data[\"text\"].apply(lambda text: function(text))\n",
    "\n",
    "data.to_csv(f\"news.2013.cs.trainlen.200.gpt2.csv\", index=False) # 18253\n",
    "# data.to_csv(f\"news.2013.cs.trainlen.200.bpe.csv\", index=False) #done\n",
    "# data.to_csv(f\"news.2013.cs.trainlen.200.char.csv\", index=False) # 18245\n",
    "# data.to_csv(f\"news.2013.cs.trainlen.200.nlp.csv\", index=False) # 18247\n",
    "# data.to_csv(f\"news.2013.cs.trainlen.200.ner.csv\", index=False) # 18246"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "enigmavenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
