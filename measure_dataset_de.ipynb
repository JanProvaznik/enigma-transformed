{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from collections import Counter\n",
    "import torch\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from src.evaluation import js_divergence\n",
    "# what functions are avaliable to measure?\n",
    "\n",
    "# 1. unigram js_divergence\n",
    "# 2. bpe\n",
    "# 3. bigram js_divergence\n",
    "# 4. gpt2 perplexity\n",
    "# 5. depth of parse tree\n",
    "# 6. js_divergence of POS tags\n",
    "# 7. js_divergence of POS bigrams\n",
    "# 8. number of named entities \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_gpt2_perplexity():\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "    device = (\n",
    "        torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    )\n",
    "    gpt2 = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n",
    "\n",
    "    def gpt2_perplexity(text):\n",
    "        # Encode and prepare inputs\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        # Calculate log likelihood\n",
    "        with torch.no_grad():\n",
    "            outputs = gpt2(**inputs, labels=inputs[\"input_ids\"])\n",
    "        log_likelihood = outputs.loss.item()\n",
    "\n",
    "        # Calculate perplexity\n",
    "        perplexity = torch.exp(torch.tensor(log_likelihood)).item()\n",
    "\n",
    "        return perplexity\n",
    "    return gpt2_perplexity\n",
    "\n",
    "\n",
    "def create_bpe_tokens_per_char():\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "    def bpe_tokens_per_char(text):\n",
    "        chars = len(text)\n",
    "        tokens = len(tokenizer.encode(text))\n",
    "        return tokens / chars\n",
    "    return bpe_tokens_per_char\n",
    "\n",
    "\n",
    "def find_depth(node):\n",
    "    if not list(node.children):\n",
    "        return 1\n",
    "    else:\n",
    "        return 1 + max(find_depth(child) for child in node.children)\n",
    "\n",
    "\n",
    "def create_all_nlp_functions_de(data):\n",
    "    nlp = spacy.load(\"de_core_news_md\")\n",
    "    counts_bigram_pos = Counter()\n",
    "    counts_pos = Counter()\n",
    "    for text in data.original_text:\n",
    "        doc = nlp(text)\n",
    "        pos = [token.pos_ for token in doc]\n",
    "        counts_pos.update(pos)\n",
    "        counts_bigram_pos.update(zip(pos, pos[1:]))\n",
    "\n",
    "    def inner(text):\n",
    "        doc = nlp(text)\n",
    "        pos = [token.pos_ for token in doc]\n",
    "        pos_js_divergence = js_divergence(counts_pos, Counter(pos))\n",
    "        pos_bigram_js_divergence = js_divergence(counts_bigram_pos, Counter(zip(pos, pos[1:])))\n",
    "        root = [token for token in doc if token.head == token][0]\n",
    "        depth = find_depth(root)\n",
    "        named_entities = len(doc.ents)\n",
    "\n",
    "        \n",
    "        return pos_js_divergence, pos_bigram_js_divergence, depth, named_entities\n",
    "    return inner\n",
    "\n",
    "\n",
    "def create_char_bigram_divergences(data):\n",
    "    unigram_counts = Counter()\n",
    "    bigram_counts = Counter()\n",
    "    for text in data.text:\n",
    "        unigram_counts.update(text)\n",
    "        bigram_counts.update(zip(text, text[1:]))\n",
    "    def inner(text):\n",
    "        unigram_divergence = js_divergence(unigram_counts, Counter(text))\n",
    "        bigram_divergence = js_divergence(bigram_counts, Counter(zip(text, text[1:])))\n",
    "        return unigram_divergence, bigram_divergence\n",
    "    return inner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fn created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49/49 [00:12<00:00,  4.03it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "data = pd.read_csv(\"news.2013.de.trainlen.200.nlp.csv\")\n",
    "# data = pd.read_csv(\"news.test.de.csv\")\n",
    "available_functions = [\n",
    "    # (\"gpt2_perplexity\", create_gpt2_perplexity(), 'text'), # done\n",
    "    # (\"bpe_tokens_per_char\", create_bpe_tokens_per_char(), 'text'), # done\n",
    "    # (\"unigram_js_divergence\", create_unigram_js_divergence(data), 'text'), #done\n",
    "    # (\"bigram_js_divergence\", create_bigram_js_divergence(data), 'text'),#17648\n",
    "    # (\"pos_js_divergence\", create_pos_js_divergence(data),'original_text'), #17650\n",
    "    # (\"pos_bigram_js_divergence\", create_pos_bigram_js_divergence(data), 'original_text'), #17655\n",
    "    # (\"depth_of_parse_tree\", create_depth_of_parse_tree(), 'original_text'),#17652\n",
    "    # (\"named_entities\", create_named_entities(),'original_text')#17649\n",
    "]\n",
    "\n",
    "# compute all nlp things at once!\n",
    "# nlpfuncs= create_all_nlp_functions_de(data)\n",
    "# data[\"pos_js_divergence\"], data[\"pos_bigram_js_divergence\"], data[\"depth_of_parse_tree\"], data[\"named_entities\"] = zip(*data.original_text.apply(lambda text: nlpfuncs(text)))\n",
    "\n",
    "\n",
    "# compute char divergences\n",
    "# char_divergences = create_char_bigram_divergences(data)\n",
    "# print(\"fn created\")\n",
    "# data[\"unigram_js_divergence\"], data[\"bigram_js_divergence\"] = zip(*data.text.apply(lambda text: char_divergences(text)))\n",
    "\n",
    "# compute gpt2 perplexity\n",
    "gpt2_perplexity = create_gpt2_perplexity()\n",
    "print(\"fn created\")\n",
    "\n",
    "\n",
    "data[\"gpt2_perplexity\"] = data.text.progress_apply(lambda text: gpt2_perplexity(text))\n",
    "\n",
    "# compute bpe tokens per char\n",
    "# bpe_tokens_per_char = create_bpe_tokens_per_char()\n",
    "# print(\"fn created\")\n",
    "# data[\"bpe_tokens_per_char\"] = data.text.apply(lambda text: bpe_tokens_per_char(text))\n",
    "\n",
    "# print data\n",
    "# processing_now=0\n",
    "# fn_name, function, src_col = available_functions[processing_now]\n",
    "# print(f\"Processing {fn_name}\")\n",
    "\n",
    "# function = bpe_tokens_per_char\n",
    "# data[fn_name] = data[src_col].apply(lambda text: function(text))\n",
    "# data[\"gpt2_tokens_per_char\"] = data[\"text\"].apply(lambda text: function(text))\n",
    "\n",
    "# data.to_csv(f\"news.2013.de.trainlen.200.nlp.csv\", index=False)\n",
    "# data.to_csv(f\"news.2013.de.trainlen.200.char.csv\", index=False)\n",
    "data.to_csv(f\"news.2013.de.trainlen.200.gpt2.csv\", index=False) #\n",
    "# data.to_csv(f\"news.2013.de.trainlen.200.bpe.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "enigmavenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
