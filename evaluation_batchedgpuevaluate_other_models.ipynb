{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inference "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data\n",
    "## this is for the case where we already have the clean dataset from before and just want to evaluate other models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. load dataset to csv\n",
    "2. load the model \n",
    "3. run the model function on each sentence in dataset...\n",
    "4. batch this process\n",
    "5. hope for no race condition when saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n"
     ]
    }
   ],
   "source": [
    "print(\"start\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_max_len = 200\n",
    "len_of_dataset = 100000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded\n",
      "Model(dataset_class=<class 'src.ByT5Dataset.ByT5NoisyVignere3Dataset'>, name='en_noisevignere3_500', language='en', slurm_id=22819, is_checkpoint=True, checkpoint_step=500, noise_proportion=0.15)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from src.ByT5Dataset import ByT5ConstEnigmaDataset, ByT5CaesarRandomDataset, ByT5NoisyVignere2Dataset, ByT5NoisyConstEnigmaDataset, ByT5NoisyVignere3Dataset\n",
    "from src.evaluation import Model\n",
    "from src.ByT5Dataset import ByT5Dataset\n",
    "import argparse\n",
    "\n",
    "models = {\n",
    "    'caesar': Model(ByT5CaesarRandomDataset, 'caesar', 'en', 16677),\n",
    "    'en_constenigma': Model(ByT5ConstEnigmaDataset, 'en_constenigma', 'en', 17510),\n",
    "    'de_constenigma': Model(ByT5ConstEnigmaDataset, 'de_constenigma', 'de', 18065),\n",
    "    'cs_constenigma': Model(ByT5ConstEnigmaDataset, 'cs_constenigma', 'cs', 18066),\n",
    "    # v1\n",
    "    'en_noisevignere_checkpoint-5000': Model(ByT5NoisyVignere2Dataset, 'en_noisevignere_checkpoint-5000', 'en', 20145, True, 5000, .15), #20232\n",
    "    'en_noisevignere_checkpoint-10000': Model(ByT5NoisyVignere2Dataset, 'en_noisevignere_checkpoint-10000', 'en', 20145, True, 10000, .15), #20233\n",
    "    'en_noisevignere_checkpoint-15000': Model(ByT5NoisyVignere2Dataset, 'en_noisevignere_checkpoint-15000', 'en', 20145, True, 15000, .15), #20451\n",
    "    'en_noisevignere_checkpoint-20000': Model(ByT5NoisyVignere2Dataset, 'en_noisevignere_checkpoint-20000', 'en', 20145, True, 20000, .15), # 20498\n",
    "    'en_noisy_const_enigma_checkpoint-5000': Model(ByT5NoisyConstEnigmaDataset, 'en_noisyconsteenigma_checkpoint-5000', 'en', 20195, True, 5000, .2), # 20845\n",
    "    'en_noisy_const_enigma_checkpoint-10000': Model(ByT5NoisyConstEnigmaDataset, 'en_noisyconsteenigma_checkpoint-10000', 'en', 20195, True, 10000, .2), # 20846\n",
    "    'en_noisy_const_enigma_checkpoint-15000': Model(ByT5NoisyConstEnigmaDataset, 'en_noisyconsteenigma_checkpoint-15000', 'en', 20195, True, 15000, .2), # 20847\n",
    "    'en_noisy_const_enigma_checkpoint-20000': Model(ByT5NoisyConstEnigmaDataset, 'en_noisyconsteenigma_checkpoint-20000', 'en', 20195, True, 20000, .2), # 20848\n",
    "\n",
    "    # v2\n",
    "    \n",
    "    'en_noisevignere3_500': Model(ByT5NoisyVignere3Dataset, 'en_noisevignere3_500', 'en', 22819, True, 500, .15), # 22893\n",
    "    'en_noisevignere3_1000': Model(ByT5NoisyVignere3Dataset, 'en_noisevignere3_1000', 'en', 22819, True, 1000, .15), # 22894\n",
    "    'en_noisevignere3_1500': Model(ByT5NoisyVignere3Dataset, 'en_noisevignere3_1500', 'en', 22819, True, 1500, .15), # 22895\n",
    "    'en_noisevignere3_2000': Model(ByT5NoisyVignere3Dataset, 'en_noisevignere3_2000', 'en', 22819, True, 2000, .15), # 22896\n",
    "    'en_noisevignere3_2500': Model(ByT5NoisyVignere3Dataset, 'en_noisevignere3_2500', 'en', 22819, True, 2500, .15), # 22897\n",
    "    'en_noisevignere3_3000': Model(ByT5NoisyVignere3Dataset, 'en_noisevignere3_3000', 'en', 22819, True, 3000, .15), # 22898\n",
    "    'en_noisevignere3_3500': Model(ByT5NoisyVignere3Dataset, 'en_noisevignere3_3500', 'en', 22819, True, 3500, .15), # 22899\n",
    "    'en_noisevignere3_4000': Model(ByT5NoisyVignere3Dataset, 'en_noisevignere3_4000', 'en', 22819, True, 4000, .15), # 22900\n",
    "\n",
    "    # de 22904\n",
    "    'de_noisevignere3_500': Model(ByT5NoisyVignere3Dataset, 'de_noisevignere3_500', 'de', 22904 , True, 500, .15), # 23104\n",
    "    'de_noisevignere3_1000': Model(ByT5NoisyVignere3Dataset, 'de_noisevignere3_1000', 'de', 22904 , True, 1000, .15), # 23105\n",
    "    'de_noisevignere3_1500': Model(ByT5NoisyVignere3Dataset, 'de_noisevignere3_1500', 'de', 22904 , True, 1500, .15), # 23150\n",
    "    'de_noisevignere3_2000': Model(ByT5NoisyVignere3Dataset, 'de_noisevignere3_2000', 'de', 22904 , True, 2000, .15), # 23161\n",
    "    'de_noisevignere3_2500': Model(ByT5NoisyVignere3Dataset, 'de_noisevignere3_2500', 'de', 22904 , True, 2500, .15), # 23162\n",
    "    'de_noisevignere3_3000': Model(ByT5NoisyVignere3Dataset, 'de_noisevignere3_3000', 'de', 22904 , True, 3000, .15), # 23163\n",
    "    'de_noisevignere3_3500': Model(ByT5NoisyVignere3Dataset, 'de_noisevignere3_3500', 'de', 22904 , True, 3500, .15), # 23164\n",
    "    'de_noisevignere3_4000': Model(ByT5NoisyVignere3Dataset, 'de_noisevignere3_4000', 'de', 22904 , True, 4000, .15), # 23165\n",
    "\n",
    "    # cs 22989\n",
    "    'cs_noisevignere3_500': Model(ByT5NoisyVignere3Dataset, 'cs_noisevignere3_500', 'cs', 22989 , True, 500, .15), # 23166\n",
    "    'cs_noisevignere3_1000': Model(ByT5NoisyVignere3Dataset, 'cs_noisevignere3_1000', 'cs', 22989 , True, 1000, .15), # 23168\n",
    "    'cs_noisevignere3_1500': Model(ByT5NoisyVignere3Dataset, 'cs_noisevignere3_1500', 'cs', 22989 , True, 1500, .15), # 23169\n",
    "    'cs_noisevignere3_2000': Model(ByT5NoisyVignere3Dataset, 'cs_noisevignere3_2000', 'cs', 22989 , True, 2000, .15), # 23170\n",
    "    'cs_noisevignere3_2500': Model(ByT5NoisyVignere3Dataset, 'cs_noisevignere3_2500', 'cs', 22989 , True, 2500, .15), # 23171\n",
    "    'cs_noisevignere3_3000': Model(ByT5NoisyVignere3Dataset, 'cs_noisevignere3_3000', 'cs', 22989 , True, 3000, .15), # 23177\n",
    "    'cs_noisevignere3_3500': Model(ByT5NoisyVignere3Dataset, 'cs_noisevignere3_3500', 'cs', 22989 , True, 3500, .15), # 23178\n",
    "    'cs_noisevignere3_4000': Model(ByT5NoisyVignere3Dataset, 'cs_noisevignere3_4000', 'cs', 22989 , True, 4000, .15), # 23179\n",
    "    # enigmas\n",
    "\n",
    "    # de enigma 23190\n",
    "    'de_noiseconstenigma_500' : Model(ByT5NoisyConstEnigmaDataset, 'de_noiseconstenigma_500' , 'de', 23190 , True, 500 , .15), # 23639\n",
    "    'de_noiseconstenigma_1000': Model(ByT5NoisyConstEnigmaDataset, 'de_noiseconstenigma_1000', 'de', 23190 , True, 1000, .15), # 23640\n",
    "    'de_noiseconstenigma_1500': Model(ByT5NoisyConstEnigmaDataset, 'de_noiseconstenigma_1500', 'de', 23190 , True, 1500, .15), # 23641\n",
    "    'de_noiseconstenigma_2000': Model(ByT5NoisyConstEnigmaDataset, 'de_noiseconstenigma_2000', 'de', 23190 , True, 2000, .15), # 23642\n",
    "    'de_noiseconstenigma_2500': Model(ByT5NoisyConstEnigmaDataset, 'de_noiseconstenigma_2500', 'de', 23190 , True, 2500, .15), # 23643\n",
    "    'de_noiseconstenigma_3000': Model(ByT5NoisyConstEnigmaDataset, 'de_noiseconstenigma_3000', 'de', 23190 , True, 3000, .15), # 23644\n",
    "    'de_noiseconstenigma_3500': Model(ByT5NoisyConstEnigmaDataset, 'de_noiseconstenigma_3500', 'de', 23190 , True, 3500, .15), # 23645\n",
    "    'de_noiseconstenigma_4000': Model(ByT5NoisyConstEnigmaDataset, 'de_noiseconstenigma_4000', 'de', 23190 , True, 4000, .15), # 23646\n",
    "\n",
    "    # cs enigma 23167 \n",
    "    'cs_noiseconstenigma_500': Model(ByT5NoisyConstEnigmaDataset, 'cs_noiseconstenigma_500', 'cs', 23167 , True, 500, .15),    # 23647\n",
    "    'cs_noiseconstenigma_1000': Model(ByT5NoisyConstEnigmaDataset, 'cs_noiseconstenigma_1000', 'cs', 23167 , True, 1000, .15), # 23648\n",
    "    'cs_noiseconstenigma_1500': Model(ByT5NoisyConstEnigmaDataset, 'cs_noiseconstenigma_1500', 'cs', 23167 , True, 1500, .15), # 23649\n",
    "    'cs_noiseconstenigma_2000': Model(ByT5NoisyConstEnigmaDataset, 'cs_noiseconstenigma_2000', 'cs', 23167 , True, 2000, .15), # 23650\n",
    "    'cs_noiseconstenigma_2500': Model(ByT5NoisyConstEnigmaDataset, 'cs_noiseconstenigma_2500', 'cs', 23167 , True, 2500, .15), # 23651\n",
    "    'cs_noiseconstenigma_3000': Model(ByT5NoisyConstEnigmaDataset, 'cs_noiseconstenigma_3000', 'cs', 23167 , True, 3000, .15), # 23652\n",
    "    'cs_noiseconstenigma_3500': Model(ByT5NoisyConstEnigmaDataset, 'cs_noiseconstenigma_3500', 'cs', 23167 , True, 3500, .15), # 23653\n",
    "    'cs_noiseconstenigma_4000': Model(ByT5NoisyConstEnigmaDataset, 'cs_noiseconstenigma_4000', 'cs', 23167 , True, 4000, .15), # 23654\n",
    "\n",
    "    # en enigma 23609\n",
    "    'en_noiseconstenigma_500': Model(ByT5NoisyConstEnigmaDataset, 'en_noiseconstenigma_500', 'en', 23609 , True, 500, .15), # 24303\n",
    "    'en_noiseconstenigma_1000': Model(ByT5NoisyConstEnigmaDataset, 'en_noiseconstenigma_1000', 'en', 23609 , True, 1000, .15), # 24304\n",
    "    'en_noiseconstenigma_1500': Model(ByT5NoisyConstEnigmaDataset, 'en_noiseconstenigma_1500', 'en', 23609 , True, 1500, .15), # 24306\n",
    "    'en_noiseconstenigma_2000': Model(ByT5NoisyConstEnigmaDataset, 'en_noiseconstenigma_2000', 'en', 23609 , True, 2000, .15), # 24307\n",
    "    'en_noiseconstenigma_2500': Model(ByT5NoisyConstEnigmaDataset, 'en_noiseconstenigma_2500', 'en', 23609 , True, 2500, .15), # 24309\n",
    "    'en_noiseconstenigma_3000': Model(ByT5NoisyConstEnigmaDataset, 'en_noiseconstenigma_3000', 'en', 23609 , True, 3000, .15), # 24310\n",
    "    'en_noiseconstenigma_3500': Model(ByT5NoisyConstEnigmaDataset, 'en_noiseconstenigma_3500', 'en', 23609 , True, 3500, .15), # 24311\n",
    "    'en_noiseconstenigma_4000': Model(ByT5NoisyConstEnigmaDataset, 'en_noiseconstenigma_4000', 'en', 23609 , True, 4000, .15), # 24312\n",
    "\n",
    "\n",
    "}\n",
    "\n",
    "# evaluated_name = 'en_noisevignere_checkpoint-5000'\n",
    "# evaluated_name = 'cs_noisevignere3_4000'\n",
    "# Create an argument parser\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--eval_model', type=str, help='Name of the evaluated model')\n",
    "\n",
    "args, _ = parser.parse_known_args()\n",
    "\n",
    "# Get the evaluated name from script arguments\n",
    "evaluated_name = args.eval_model\n",
    "\n",
    "\n",
    "model_metadata = models[evaluated_name]\n",
    "\n",
    "data_path = f'news.2013.{model_metadata.language}.trainlen.200.evaluation.100000.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "print(\"data loaded\")\n",
    "print(model_metadata)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ByT5Tokenizer, T5ForConditionalGeneration\n",
    "from src.utils import levensthein_distance, print_avg_median_mode_error\n",
    "from transformers import pipeline, logging\n",
    "from src.ByT5Dataset import ByT5CaesarRandomDataset, ByT5ConstEnigmaDataset\n",
    "import torch\n",
    "import time\n",
    "\n",
    "logging.set_verbosity(logging.ERROR)\n",
    "\n",
    "tokenizer = ByT5Tokenizer()\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_metadata.path())\n",
    "model = model.half()\n",
    "model.to(device) # type: ignore\n",
    "\n",
    "dataset_class = model_metadata.dataset_class\n",
    "if model_metadata.noise_proportion is not None:\n",
    "    dataset = dataset_class(data=data.text, max_length=dataset_max_len, noise_proportion=model_metadata.noise_proportion) # type: ignore , not sound but will work in my usecase\n",
    "else:\n",
    "    dataset = dataset_class(data=data.text, max_length=dataset_max_len) # type: ignore , not sound but will work in my usecase\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# translate = pipeline(\"translation\", model=model, tokenizer=tokenizer, device=device)\n",
    "\n",
    "error_col_name = f'{model_metadata.name}_error_count'\n",
    "generated_text_col_name = f'{model_metadata.name}_generated_text'\n",
    "\n",
    "data[error_col_name] = 0\n",
    "data[generated_text_col_name] = ''\n",
    "\n",
    "batch_size = 512\n",
    "data = data.reset_index(drop=True)\n",
    "for i in range(0, len(dataset), batch_size):\n",
    "    t0 = time.time()\n",
    "    batch = dataset[i:i+batch_size]\n",
    "    input_texts = batch['input_text'] \n",
    "    output_texts = batch['output_text'] \n",
    "    inputs = tokenizer(input_texts, return_tensors=\"pt\", padding=True,max_length=dataset_max_len,truncation=True).to(device)\n",
    "\n",
    "    # Generate translations in batches\n",
    "    tgpu0=time.time()\n",
    "    with torch.cuda.amp.autocast():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=dataset_max_len)\n",
    "    tgpu1=time.time()\n",
    "    generated_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    # generated_texts = translate(input_texts, max_length=(dataset_max_len + 1) * 2)\n",
    "    # generated_texts = [t['translation_text'] for t in generated_texts] # type: ignore \n",
    "\n",
    "    # Calculate errors and update DataFrame\n",
    "    errors = [levensthein_distance(gen, out) for gen, out in zip(generated_texts, output_texts)]\n",
    "    data.loc[i:i+batch_size-1, generated_text_col_name] = generated_texts\n",
    "    data.loc[i:i+batch_size-1, error_col_name] = errors\n",
    "    t1 = time.time()\n",
    "\n",
    "    print(f\"Processed batch {i // batch_size + 1}/{len(dataset) // batch_size} in {t1 - t0:.2f} seconds\")\n",
    "    print(f\"model gen time: {tgpu1 - tgpu0:.2f} seconds\")\n",
    "\n",
    "\n",
    "avg, med, mode = print_avg_median_mode_error(data[error_col_name].tolist())\n",
    "print(\"#############################################\")\n",
    "\n",
    "print(\"avg:\", avg)\n",
    "print(\"med:\", med)\n",
    "print(\"mode:\", mode)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data.to_csv(f\"news.2013.{model_metadata.language}.trainlen.200.evaluation.100000.intermediate.{model_metadata.name}.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
